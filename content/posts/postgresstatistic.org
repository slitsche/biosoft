#+TITLE: Why Tune Analyze in PostgreSQL
#+DATE: 2020-01-09T21:06:34+01:00
#+TAGS[]: PostgreSQL
#+DRAFT: true

Why is it important to adjust the statistics target for big tables?

With the trend in the business to automate processes using decisions based on
data about the business there comes the challenge for data engineers to deal
with more and more data.  Furthermore the information systems should behave
reliably and respond in a predictable manner.  If you are working with
PostgreSQL it is the question how to configure the system to be able to
process queries so that the service level objectives are met.

In a [[/posts/analyze-extreme-distributions-in-postgresql/][previous post]] I analyzed the issue for a very specific case: few values
(low cardinality) in a big table with a very uneven distribution.  In this post
I want to discuss the influence of uneven distribution for /high cardinality/
columns on the quality of query planning.

Let's imagine we have a table with a non unique high cardinality column.
Additionally let's assume a highly uneven distribution.  As we learn from [[https://www.postgresql.org/docs/12/planner-stats.html][the
documentation]] the maximum number of entries for ~histogram_bounds~ and
~most_common_vals~ (also refered to as MCV) are configured by the parameter
~statistics~ that could be set per column.  The default is ~100~.

If there are /not more/ than 100 high frequent values the default configuration
will work just fine.  Up to 100 frequent values we will see represented in
the MCV list.  The majority of values will be represented using
[[https://www.postgresql.org/docs/12/view-pg-stats.html][~histogram_bounds~]].

[[/pg_stat_highcard/pg_stat1.png]]

*Figure 1:* As long as the most frequent values are all covered when we reach
the more flatten part of the distribution curve query planning should be fine.

But what happens if there are more values with a frequency in the steep part of
the distribution graph?

In order to illustrate it you see an example with less data.

In the picture we see an uneven distribution.  A column in our table
might have the 10 different values (from 1 to 10).  From all rows 58% have the
value "9".



Every bucket in ~histogram_bounds~ shall represent the same number of rows.
Therefore we could think of the borders of the bucket as a percentile.  If we
consider the case of 50% percentile, we get 2 buckets each of it the
representing 50% of the rows.  In that case the borders of our buckets are 9
and 10.

The planner uses such percentiles to estimate the affected number of rows for a
given predicate.  This is described in detail in the [[https://www.postgresql.org/docs/current/row-estimation-examples.html][PostgreSQL documentation]].

There we learn that for values in the histogram buckets we assume uniform
distribution.  So we will estimate the same selectivity or the same number of
rows for every value from 1 to 9.  This leads to estimations much bigger than
the actual row numbers for our values 1 to 5 (which have quite small
frequency).  Also the frequency for 10 will be overestimated.

How could we improve the estimation?  In our example we increase the number of
buckets from 2 to 8.


TODO:  real world example for such a distribution in the histogram bounds?

We know it has different effects:

- increase the sample size
- increase the number of buckets
- increase the number of entries in MCV
