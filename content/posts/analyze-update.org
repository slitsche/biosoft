#+TITLE: An Update on Statistics Target
#+DATE: 2019-12-01T10:30:00+01:00
#+TAGS[]: PostgreSQL
#+DRAFT: false

In an [[/posts/analyze-extreme-distributions-in-postgresql/][earlier article I analyzed]] the influence of the statistics target on the
result of sampling for extreme distributions.  The representation of extreme
rare values in the [[https://www.postgresql.org/docs/current/view-pg-stats.html ][most common values]] required a drastic increase of the sample
size.

My colleage Alex Shulgin initiated a patch which improved the situation for
null values.  In PostgreSQL 9.6 the [[https://www.postgresql.org/docs/release/9.6.0/][improvements for =analyze=]] were released.
More work was done on this issue later to improve the [[https://www.postgresql.org/docs/release/11.0/][selection of most common
values]] which was released in PostgreSQL 11.

I was curious how the situation has changed.  Which values for the statistics
target should I choose in a similar situation?  So I repeated the analysis with
a newer version of Postgres.  Below you find the results for PostgeSQL 11.2 (10
Mio rows, 10 samples for =analyze=).

[[file:/extreme/Postgres11.2.png]]

Now the graphs are monotonic.  What an achievement!
